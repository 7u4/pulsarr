#!/usr/bin/env bash
#
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# 
# The Crawl command script : crawl <seedDir> <crawlId> <solrURL> <numberOfRounds>
#
# 
# UNLIKE THE PULSAR ALL-IN-ONE-CRAWL COMMAND THIS SCRIPT DOES THE LINK INVERSION AND
# INDEXING FOR EACH BATCH

bin=$(dirname "$0")
bin=$(cd "$bin">/dev/null || exit; pwd)

 . "$bin"/include/pulsar-config.sh
 . "$bin"/include/pulsar-common.sh

bin="$PULSAR_HOME"/bin

#############################################
# MODIFY THE PARAMETERS BELOW TO YOUR NEEDS #
#############################################
# set max depth, no more links are collected in deeper pages
maxDepth=1
minAnchorLen=5
maxAnchorLen=40
ignoreExternalLinks=true
# max out links per page collected each time
maxLinks=500
# index pages just after it's fetched
indexJIT=false
# Adds <days> to the current time to facilitate
# crawling urls already fetched sooner then
# db.default.fetch.interval.
addDays=0

# set the number of slaves nodes
numSlaves=1
if [ -n "$NUMBER_SLAVES" ]; then
 numSlaves=$NUMBER_SLAVES
fi

# and the total number of available tasks
# sets Hadoop parameter "mapreduce.job.reduces"
numTasks=$(expr "$numSlaves" \* 2)

# number of urls to fetch in one iteration
# It's depend on how fast do you want to finish the fetch loop
batchSize=$(expr "$numTasks" \* 1000)
if [ -n "$BATCH_SIZE" ]; then
 batchSize=$BATCH_SIZE
fi

#############################################

function printUsage() {
  echo "Usage: crawl.sh [options...] <confDir> <seeds> <crawlID> [<indexer>] <numberOfRounds>"

  echo
  echo "Options: "
  echo "  -E, --enterprise Use enterprise edition"
  echo "      --rotate-log Rotate logs"
  echo "  -v, --verbose    Talk more"
  echo "  -h, --help       The help text"
  exit 1
}

while [ $# -gt 0 ]
do
case $1 in
    -E|--enterprise)
        export ENTERPRISE_EDITION=true
        shift
        ;;
    -v|--verbose)
        export VERBOSE_LEVEL=1
        shift
        ;;
    -h|--help)
        SHOW_HELP=true
        shift
        ;;
    -*)
        echo "Unrecognized option : $1"
        echo "Try 'pulsar --help' for more information."
        exit 0
        ;;
    *)
        break
        ;;
esac
done

if [[ $SHOW_HELP ]]; then
  printUsage
  exit 0
fi

if [ "$#" -lt 4 ]; then
  printUsage
  exit 1
fi

CONF_DIR="$1"
shift
SEEDS="$1"
shift
CRAWL_ID="$1"
shift
LIMIT="$1"
shift
if [ "$#" -eq 1 ]; then
   INDEXER="$LIMIT"
   LIMIT="$1"
fi

if [ "$#" -gt 1 ]; then
  printUsage
  exit 1
fi

if [ "$PULSARJOB" = "" ]; then
  export PULSARJOB="$bin/pulsarjob"
fi

if [[ $CONF_DIR != "default" ]]; then
  export PULSAR_EXTRA_CONF_DIR="$CONF_DIR"
fi

PULSAR_SCRIPT_OPTIONS=()
if [ "$ENTERPRISE_EDITION" == "true" ]; then
  PULSAR_SCRIPT_OPTIONS=(${PULSAR_SCRIPT_OPTIONS[@]} --enterprise)
fi

if [ "$VERBOSE_LEVEL" == "1" ]; then
  PULSAR_SCRIPT_OPTIONS=(${PULSAR_SCRIPT_OPTIONS[@]} --verbose)
fi

# note that some of the options listed here could be set in the
# corresponding hadoop site xml param file
COMMAND_OPTIONS=(
    "-D mapreduce.job.reduces=$numTasks" # TODO : not suitable for all jobs
    "-D crawl.max.distance=$maxDepth"
    "-D parse.max.links=$maxLinks"
    "-D parse.min.anchor.length=$minAnchorLen"
    "-D parse.max.anchor.length=$maxAnchorLen"
    "-D parse.ignore.external.links=$ignoreExternalLinks"
    "-D mapred.child.java.opts=-Xmx1000m"
    "-D mapreduce.reduce.speculative=false"
    "-D mapreduce.map.speculative=false"
    "-D mapreduce.map.output.compress=true"
)

if [ ! -e "$PULSAR_HOME/logs" ]; then
  mkdir "$PULSAR_HOME/logs"
fi

# determines whether mode based on presence of job file
if [ "${PULSAR_RUNTIME_MODE}" == "DISTRIBUTE" ]; then
  # check that hadoop can be found on the path
  if [ $(which hadoop | wc -l ) -eq 0 ]; then
    echo "Can't find Hadoop executable. Add HADOOP_HOME/bin to the path or run in local mode."
    exit
  fi
fi

# start pulsar master as daemon
if ( ! __check_master_available ); then
#  if [ ${PULSAR_RUNTIME_MODE} == "DISTRIBUTE" ]; then
#    echo "Running under hadoop, but PMaster is not detected, exit."
#    exit 0
#  else
#    . "$bin"/start-pulsar.sh
#  fi
    # . "$bin"/start-pulsar.sh
    echo "Pulsar master is not unavailable for now"
fi

# initial injection
# echo "Injecting seed URLs"
if [[ $SEEDS != "false" ]]; then
  __run_pulsar_job "${PULSAR_SCRIPT_OPTIONS[@]}" inject "$SEEDS" -crawlId "$CRAWL_ID"
fi

# main loop : rounds of generate - fetch - parse - update
for ((a=1; a <= LIMIT ; a++))
do
  if ( __is_crawl_loop_stopped ); then
     echo "STOP file found - escaping loop"
     exit 0
  fi

#  if ( ! __check_index_server_available ); then
#    exit 1
#  fi

  echo -e "\n\n\n"
  echo "$(date)" ": Iteration $a of $LIMIT"

  batchId=$(date +%s)-$RANDOM

  echo "Generating : "
  # generate_args=(${commonOptions[@]}  "-D crawl.round=$a" -batchId $batchId -reGen -topN $batchSize -adddays $addDays -crawlId "$CRAWL_ID")
  __run_pulsar_job "${PULSAR_SCRIPT_OPTIONS[@]}" generate -D crawl.round="$a" "${COMMAND_OPTIONS[@]}" -batchId "$batchId" -reGen -topN ${batchSize} -adddays ${addDays} -crawlId "$CRAWL_ID"

  if [[ "$indexJIT" == "true" ]]; then
  echo "Fetching : "
    if [[ "$INDEX" == "" ]]; then
      __run_pulsar_job "${PULSAR_SCRIPT_OPTIONS[@]}" fetch -D crawl.round="$a" "${COMMAND_OPTIONS[@]}" "$batchId" -resume -strictDf -crawlId "$CRAWL_ID"
    else
      __run_pulsar_job "${PULSAR_SCRIPT_OPTIONS[@]}" fetch -D crawl.round="$a" "${COMMAND_OPTIONS[@]}" "$batchId" -resume -strictDf -index -indexer "$INDEXER" -crawlId "$CRAWL_ID"
    fi
  else
    echo "Fetching : "
    __run_pulsar_job "${PULSAR_SCRIPT_OPTIONS[@]}" fetch -D crawl.round="$a" "${COMMAND_OPTIONS[@]}" "$batchId" -resume -strictDf -crawlId "$CRAWL_ID"

    echo "Parsing : "
    __run_pulsar_job "${PULSAR_SCRIPT_OPTIONS[@]}" parse -D crawl.round="$a" "${COMMAND_OPTIONS[@]}" "$batchId" -crawlId "$CRAWL_ID"

    echo "Indexing : "
    __run_pulsar_job "${PULSAR_SCRIPT_OPTIONS[@]}" index -D crawl.round="$a" "${COMMAND_OPTIONS[@]}" "$batchId" -indexer "$INDEXER" -crawlId "$CRAWL_ID"
  fi

  echo "Updating outgoing pages : "
  __run_pulsar_job "${PULSAR_SCRIPT_OPTIONS[@]}" updateoutgraph -D crawl.round="$a" "${COMMAND_OPTIONS[@]}" "$batchId" -crawlId "$CRAWL_ID"

  echo "Updating incoming pages : "
  __run_pulsar_job "${PULSAR_SCRIPT_OPTIONS[@]}" updateingraph -D crawl.round="$a" "${COMMAND_OPTIONS[@]}" "$batchId" -crawlId "$CRAWL_ID"

done

exit 0
