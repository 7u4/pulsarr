= Pulsar README

Pulsar is an open source solution to scrape web data at scale.

Extracting web data at scale is extremely hard. Websites change frequently and are becoming more complex, meaning web data collected is often inaccurate or incomplete, pulsar is an open source solution to address such issues.

Pulsar supports the Network As A Database paradigm, so we can turn the Web into tables and charts using simple SQLs, and we can query the web using SQL directly.

We also have a plan to release an advanced AI to automatically extract every field in webpages with notable accuracy.

== Features

* Web spider: browser rendering, ajax data crawling
* Performance: highly optimized, rendering hundreds of pages in parallel on a single machine without be blocked
* Data quantity assurance: smart retry, accurate scheduling, web data lifetime management
* Large scale: fully distributed, designed for large scale crawling
* Simple API: single line of code to scrape, or single SQL to turn a website into a table
* X-SQL: extend SQL to manage web data: Web crawling, scraping, Web content mining, Web BI
* Bot stealth: IP rotation, web driver stealth, never get banned
* RPA: imitating human behaviors, SPA crawling, or do something else awesome
* Big data: various backend storage support: MongoDB/HBase/Gora
* Logs &amp; metrics: monitored closely and every event is recorded

For more information check http://platon.ai[platon.ai]

== Use pulsar as a library
The simplest way to leverage the power of pulsar is adding it to your project as a library.

Maven:
[source,xml]
----
<dependency>
  <groupId>ai.platon.pulsar</groupId>
  <artifactId>pulsar-all</artifactId>
  <version>1.9.1</version>
</dependency>
----

Gradle:
[source,kotlin]
----
implementation("ai.platon.pulsar:pulsar-all:1.9.0")
----

=== Basic usage:

==== Kotlin

[source,kotlin]
----
val url = "https://list.jd.com/list.html?cat=652,12345,12349"
val session = PulsarContexts.createSession()
// load a page, fetch it from the web if it has expired or if it's being fetched for the first time
val page = session.load(url, "-expires 1d")
// parse the page content into a Jsoup document
val document = session.parse(page)
// do something with the document
// ...
// load and parse
val document2 = session.loadDocument(url, "-expires 1d")
// do something with the document
// ...
// load all pages with links specified by -outLink
val pages = session.loadOutPages(url, "-expires 1d -itemExpires 7d -outLink a[href~=item]")
// load, parse and scrape fields
val fields = session.scrape(url, "-expires 1d", "li[data-sku]", listOf(".p-name em", ".p-price"))
// load, parse and scrape named fields
val fields2 = session.scrape(url, "-i 1d", "li[data-sku]", mapOf("name" to ".p-name em", "price" to ".p-price"))
----

The example code can be found here: link:pulsar-app/pulsar-examples/src/main/kotlin/ai/platon/pulsar/examples/BasicUsage.kt[kotlin], link:pulsar-app/pulsar-examples/src/main/java/ai/platon/pulsar/examples/BasicUsage.java[java].

=== Extract data from documents

Pulsar uses https://jsoup.org/[jsoup] to extract data from html documents. Jsoup implements the https://whatwg.org/html[WHATWG HTML5] specification, and parses HTML to the same DOM as modern browsers do. Check https://jsoup.org/cookbook/extracting-data/selector-syntax[selector-syntax] for the all the CSS selector syntax supported.

==== Kotlin

[source,kotlin]
----
val document = session.loadDocument(url, "-expires 1d")
val price = document.selectFirst('.price')
----

=== Scrape a massive url collection
It's really simple to scrape a massive url collection or run continuous crawls in pulsar.

==== Kotlin

[source,kotlin]
----
fun main() {
    val parseHandler = { _: WebPage, document: Document ->
        // do something wonderful with the document
        println(document.title() + "\t|\t" + document.baseUri())
    }
    val urls = LinkExtractors.fromResource("seeds.txt")
        .map { ParsableHyperlink("$it -refresh", parseHandler) }
    val context = PulsarContexts.create()
    context.submitAll(urls)
    // feel free to fetch millions of urls here
    context.submitAll(urls)
    // ...
    context.await()
}
----

==== Java

[source,java]
----
public class MassiveCrawler {

    private static void onParse(WebPage page, Document document) {
        // do something wonderful with the document
        System.out.println(document.title() + "\t|\t" + document.baseUri());
    }

    public static void main(String[] args) {
        List<Hyperlink> urls = LinkExtractors.fromResource("seeds.txt")
                .stream()
                .map(seed -> new ParsableHyperlink(seed, MassiveCrawler::onParse))
                .collect(Collectors.toList());
        PulsarContext context = PulsarContexts.create();
        context.submitAll(urls);
        // feel free to fetch millions of urls here
        context.submitAll(urls);
        // ...
        context.await();
    }
}
----

The example code can be found here: link:pulsar-app/pulsar-examples/src/main/kotlin/ai/platon/pulsar/examples/MassiveCrawler.kt[kotlin], link:pulsar-app/pulsar-examples/src/main/java/ai/platon/pulsar/examples/MassiveCrawler.java[java].

=== Use X-SQL to query the web

Scrape a single page:

[source,sql]
----
select
      dom_first_text(dom, '#productTitle') as title,
      dom_first_text(dom, '#bylineInfo') as brand,
      dom_first_text(dom, '#price tr td:matches(^Price) ~ td, #corePrice_desktop tr td:matches(^Price) ~ td') as price,
      dom_first_text(dom, '#acrCustomerReviewText') as ratings,
      str_first_float(dom_first_text(dom, '#reviewsMedley .AverageCustomerReviews span:contains(out of)'), 0.0) as score
  from load_and_select('https://www.amazon.com/dp/B07C5B98V7 -i 1s -njr 3', 'body');
----

Execute the X-SQL:

[source,kotlin]
----
val context = SQLContexts.create()
context.executeQuery(sql)
----

The example code can be found here: link:pulsar-app/pulsar-examples/src/main/kotlin/ai/platon/pulsar/examples/XSQLDemo.kt[kotlin].

The result is as follows:

----
TITLE                                                   | BRAND                  | PRICE   | RATINGS       | SCORE
HUAWEI P20 Lite (32GB + 4GB RAM) 5.84" FHD+ Display ... | Visit the HUAWEI Store | $159.99 | 1,349 ratings | 4.40
----

== Run pulsar as a REST service
When the pulsar runs as a REST service, X-SQL can be used to scrape webpages or to query the web data directly at anytime, from anywhere, without opening an IDE.

=== Requirements

* Memory 4G+
* Maven 3.2+
* The latest version of the Java 11 OpenJDK
* java and jar on the PATH
* Google Chrome 90+

=== Build from source

----
git clone https://github.com/platonai/pulsar.git
cd pulsar && bin/build-run.sh
----

=== Start the pulsar server if not started

[source,shell]
----
bin/pulsar
----

=== Scrape a webpage in another terminal window

[source,shell]
----
bin/scrape.sh
----
The bash script is quite simple, just use curl to post a X-SQL:
[source,shell]
----
curl -X POST --location "http://localhost:8182/api/x/e" -H "Content-Type: text/plain" -d "
  select
      dom_base_uri(dom) as url,
      dom_first_text(dom, '#productTitle') as title,
      str_substring_after(dom_first_href(dom, '#wayfinding-breadcrumbs_container ul li:last-child a'), '&node=') as category,
      dom_first_slim_html(dom, '#bylineInfo') as brand,
      cast(dom_all_slim_htmls(dom, '#imageBlock img') as varchar) as gallery,
      dom_first_slim_html(dom, '#landingImage, #imgTagWrapperId img, #imageBlock img:expr(width > 400)') as img,
      dom_first_text(dom, '#price tr td:contains(List Price) ~ td') as listprice,
      dom_first_text(dom, '#price tr td:matches(^Price) ~ td') as price,
      str_first_float(dom_first_text(dom, '#reviewsMedley .AverageCustomerReviews span:contains(out of)'), 0.0) as score
  from load_and_select('https://www.amazon.com/dp/B07C5B98V7 -i 1d -njr 3', 'body');"
----

The example code can be found here: link:bin/scrape.sh[bash], link:pulsar-client/src/main/java/ai/platon/pulsar/client/Scraper.java[java], link:pulsar-client/src/main/kotlin/ai/platon/pulsar/client/Scraper.kt[kotlin], link:pulsar-client/src/main/php/Scraper.php[php].

The response is as follows:

[source,json]
----
{
    "uuid": "cc611841-1f2b-4b6b-bcdd-ce822d97a2ad",
    "statusCode": 200,
    "pageStatusCode": 200,
    "pageContentBytes": 1607636,
    "resultSet": [
        {
            "title": "Tara Toys Ariel Necklace Activity Set - Amazon Exclusive (51394)",
            "listprice": "$19.99",
            "price": "$12.99",
            "categories": "Toys & Games|Arts & Crafts|Craft Kits|Jewelry",
            "baseuri": "https://www.amazon.com/dp/B00BTX5926"
        }
    ],
    "pageStatus": "OK",
    "status": "OK"
}
----

== Advanced topics:

* How to scrape pages behind a login?
* How to download resources directly within a browser context?
* How to scrape a single page application (SPA)?
** Resource mode
** RPA mode
* How to crawl pagination links?
* How to crawl newly discovered links?
* How to crawl the entire website?
* How to simulate human behaviors?
* How to schedule priority tasks?
* How to drop a scheduled task?
* How to know the status of a task?
* How to know what is happening in the system?
* How to scrape amazon.com to match industrial needs?

== Compare with other solutions
* Pulsar vs selenium/puppeteer/playwright
* Pulsar vs nutch
* Pulsar vs scrapy+splash

== Technical details
* The core concepts in pulsar.
* How to rotate proxy ips?
* How to hide my bot from being detected?
* Why to simulate human behaviors?
* How to render as many pages as possible on a single machine?
