= Pulsar README
Vincent Zhang <ivincent.zhang@gmail.com>
3.0, July 29, 2022: Pulsar README
:toc:
:icons: font
:url-quickref: https://docs.asciidoctor.org/asciidoc/latest/syntax-quick-reference/

Pulsar is an open source solution to scrape web data at scale.

Extracting web data at scale is extremely hard. Websites change frequently and are becoming more complex, meaning web data collected is often inaccurate or incomplete, pulsar is an open source solution to address such issues.

Pulsar supports the Network As A Database paradigm, so we can turn the Web into tables and charts using simple SQLs, and we can query the web using SQL directly.

We also have a plan to release an advanced AI to automatically extract every field in webpages with notable accuracy.

== Features

* Web spider: browser rendering, ajax data crawling
* Performance: highly optimized, rendering hundreds of pages in parallel on a single machine without be blocked
* Data quantity assurance: smart retry, accurate scheduling, web data lifetime management
* Large scale: fully distributed, designed for large scale crawling
* Simple API: single line of code to scrape, or single SQL to turn a website into a table
* X-SQL: extended SQL to manage web data: Web crawling, scraping, Web content mining, Web BI
* Bot stealth: IP rotation, web driver stealth, never get banned
* RPA: imitating human behaviors, SPA crawling, or do something else awesome
* Big data: various backend storage support: MongoDB/HBase/Gora
* Logs &amp; metrics: monitored closely and every event is recorded

== Requirements

* Memory 4G+
* Maven 3.2+
* The latest version of the Java 11 OpenJDK
* java and jar on the PATH
* Google Chrome 90+

Pulsar is tested on Ubuntu 20.04, Windows 11, WSL, any other OS matches the requirements is also supposed be OK.

== Use Pulsar as a library
The simplest way to leverage the power of Pulsar is to add it to your project as a library.

Maven:
[source,xml]
----
<dependency>
  <groupId>ai.platon.pulsar</groupId>
  <artifactId>pulsar-all</artifactId>
  <version>1.9.3</version>
</dependency>
----

Gradle:
[source,kotlin]
----
implementation("ai.platon.pulsar:pulsar-all:1.9.3")
----

=== Basic usage

*Kotlin*

[source,kotlin]
----
// create a pulsar session
val session = PulsarContexts.createSession()
// the main url we are playing with
val url = "https://list.jd.com/list.html?cat=652,12345,12349"
// load a page, fetch it from the web if it has expired or if it's the first time to fetch
val page = session.load(url, "-expires 1d")
// parse the page content into a Jsoup document
val document = session.parse(page)
// do something with the document
// ...

// or, load and parse
val document2 = session.loadDocument(url, "-expires 1d")
// do something with the document
// ...

// load all pages with links specified by -outLink
val pages = session.loadOutPages(url, "-expires 1d -itemExpires 7d -outLink a[href~=item]")
// load, parse and scrape fields
val fields = session.scrape(url, "-expires 1d", "li[data-sku]", listOf(".p-name em", ".p-price"))
// load, parse and scrape named fields
val fields2 = session.scrape(url, "-i 1d", "li[data-sku]", mapOf("name" to ".p-name em", "price" to ".p-price"))
----

The example code can be found here: link:pulsar-app/pulsar-examples/src/main/kotlin/ai/platon/pulsar/examples/BasicUsage.kt[kotlin], link:pulsar-app/pulsar-examples/src/main/java/ai/platon/pulsar/examples/BasicUsage.java[java].

*Load options*

Note that most of our scraping methods accept a parameter called load arguments, or load options, to control how to load/fetch a webpage.

The most important load options are:

    -expires     // The expiry time of a page
    -itemExpires // The expiry time of item pages in some batch scraping methods
    -outLink     // The selector for out links to scrape
    -refresh     // Force (re)fetch the page, just like hitting the refresh button on a real browser
    -parse       // Triger the parse phrase
    -isResource  // Fetch the url as a resource without browser rendering

All load options are parsed to a link:pulsar-skeleton/src/main/kotlin/ai/platon/pulsar/common/options/LoadOptions.kt[LoadOptions], check the code for all the supported options.

=== Extract web data

Pulsar uses https://jsoup.org/[jsoup] to extract data from html documents. Jsoup implements the https://whatwg.org/html[WHATWG HTML5] specification, and parses HTML to the same DOM as modern browsers do. Check https://jsoup.org/cookbook/extracting-data/selector-syntax[selector-syntax] for all the supported CSS selectors.

*Kotlin*

[source,kotlin]
----
val document = session.loadDocument(url, "-expires 1d")
val price = document.selectFirst('.price')
----

=== Continuous crawling
It's really simple to scrape a massive url collection or run continuous crawls in Pulsar.

*Kotlin*

[source,kotlin]
----
fun main() {
    val parseHandler = { _: WebPage, document: Document ->
        // do something wonderful with the document
        println(document.title() + "\t|\t" + document.baseUri())
    }
    val urls = LinkExtractors.fromResource("seeds.txt")
        .map { ParsableHyperlink("$it -refresh", parseHandler) }
    val context = PulsarContexts.create()
    context.submitAll(urls)
    // feel free to submit millions of urls here
    context.submitAll(urls)
    // ...
    context.await()
}
----

*Java*

[source,java]
----
public class MassiveCrawler {

    private static void onParse(WebPage page, Document document) {
        // do something wonderful with the document
        System.out.println(document.title() + "\t|\t" + document.baseUri());
    }

    public static void main(String[] args) {
        List<Hyperlink> urls = LinkExtractors.fromResource("seeds.txt")
                .stream()
                .map(seed -> new ParsableHyperlink(seed, MassiveCrawler::onParse))
                .collect(Collectors.toList());
        PulsarContext context = PulsarContexts.create();
        context.submitAll(urls);
        // feel free to submit millions of urls here
        context.submitAll(urls);
        // ...
        context.await();
    }
}
----

The example code can be found here: link:pulsar-app/pulsar-examples/src/main/kotlin/ai/platon/pulsar/examples/MassiveCrawler.kt[kotlin], link:pulsar-app/pulsar-examples/src/main/java/ai/platon/pulsar/examples/MassiveCrawler.java[java].

=== Use X-SQL to query the web

Scrape a single page:

[source,sql]
----
select
      dom_first_text(dom, '#productTitle') as title,
      dom_first_text(dom, '#bylineInfo') as brand,
      dom_first_text(dom, '#price tr td:matches(^Price) ~ td, #corePrice_desktop tr td:matches(^Price) ~ td') as price,
      dom_first_text(dom, '#acrCustomerReviewText') as ratings,
      str_first_float(dom_first_text(dom, '#reviewsMedley .AverageCustomerReviews span:contains(out of)'), 0.0) as score
  from load_and_select('https://www.amazon.com/dp/B07C5B98V7 -i 1s -njr 3', 'body');
----

Execute the X-SQL:

[source,kotlin]
----
val context = SQLContexts.create()
val rs = context.executeQuery(sql)
println(ResultSetFormatter(rs, withHeader = true))
----

The result is as follows:

----
TITLE                                                   | BRAND                  | PRICE   | RATINGS       | SCORE
HUAWEI P20 Lite (32GB + 4GB RAM) 5.84" FHD+ Display ... | Visit the HUAWEI Store | $159.99 | 1,349 ratings | 4.40
----

The example code can be found here: link:pulsar-app/pulsar-examples/src/main/kotlin/ai/platon/pulsar/examples/XSQLDemo.kt[kotlin].

== Run Pulsar as a REST service
When Pulsar runs as a REST service, X-SQL can be used to scrape webpages or to query the web data directly at anytime, from anywhere, without opening an IDE.

=== Build from source
----
git clone https://github.com/platonai/pulsar.git
cd pulsar && bin/build-run.sh
----
For Chinese developers, we strongly suggest you to follow link:bin/tools/maven/maven-settings.adoc[this] instruction to accelerate the building.

=== Use X-SQL to query the web

Start the pulsar server if not started:

[source,shell]
----
bin/pulsar
----

Scrape a webpage in another terminal window:

[source,shell]
----
bin/scrape.sh
----
The bash script is quite simple, just use curl to post an X-SQL:
[source,shell]
----
curl -X POST --location "http://localhost:8182/api/x/e" -H "Content-Type: text/plain" -d "
  select
      dom_base_uri(dom) as url,
      dom_first_text(dom, '#productTitle') as title,
      str_substring_after(dom_first_href(dom, '#wayfinding-breadcrumbs_container ul li:last-child a'), '&node=') as category,
      dom_first_slim_html(dom, '#bylineInfo') as brand,
      cast(dom_all_slim_htmls(dom, '#imageBlock img') as varchar) as gallery,
      dom_first_slim_html(dom, '#landingImage, #imgTagWrapperId img, #imageBlock img:expr(width > 400)') as img,
      dom_first_text(dom, '#price tr td:contains(List Price) ~ td') as listprice,
      dom_first_text(dom, '#price tr td:matches(^Price) ~ td') as price,
      str_first_float(dom_first_text(dom, '#reviewsMedley .AverageCustomerReviews span:contains(out of)'), 0.0) as score
  from load_and_select('https://www.amazon.com/dp/B07C5B98V7 -i 1d -njr 3', 'body');"
----

The example code can be found here: link:bin/scrape.sh[bash], link:bin/scrape.bat[batch], link:pulsar-client/src/main/java/ai/platon/pulsar/client/Scraper.java[java], link:pulsar-client/src/main/kotlin/ai/platon/pulsar/client/Scraper.kt[kotlin], link:pulsar-client/src/main/php/Scraper.php[php].

The response is as follows in json format:

[source,json]
----
{
    "uuid": "cc611841-1f2b-4b6b-bcdd-ce822d97a2ad",
    "statusCode": 200,
    "pageStatusCode": 200,
    "pageContentBytes": 1607636,
    "resultSet": [
        {
            "title": "Tara Toys Ariel Necklace Activity Set - Amazon Exclusive (51394)",
            "listprice": "$19.99",
            "price": "$12.99",
            "categories": "Toys & Games|Arts & Crafts|Craft Kits|Jewelry",
            "baseuri": "https://www.amazon.com/dp/B00BTX5926"
        }
    ],
    "pageStatus": "OK",
    "status": "OK"
}
----

== Advanced topics
* How to scrape a million product pages from an e-commerce website a day?
* How to scrape pages behind a login?
* How to download resources directly within a browser context?
* How to scrape a single page application (SPA)?
** Resource mode
** RPA mode
* How to make sure all fields are extracted correctly?
* How to crawl paginated links?
* How to crawl newly discovered links?
* How to crawl the entire website?
* How to simulate human behaviors?
* How to schedule priority tasks?
* How to start a task at a fixed time point?
* How to drop a scheduled task?
* How to know the status of a task?
* How to know what is happening in the system?
* How to automatically generate the css selectors for fields to scrape?
* How to extract content from websites using machine learning automatically with commercial accuracy?
* How to scrape amazon.com to match industrial needs?

== Compare with other solutions
* Pulsar vs selenium/puppeteer/playwright
* Pulsar vs nutch
* Pulsar vs scrapy+splash

== Technical details
* The core concepts of Pulsar
** Web Scraping
** Network As A Database
** Browser Rendering
** Headless Browser
** Hyperlink
** Load Options
** Event Handler
** X-SQL
** Auto Web Mining
* More concepts of Pulsar
** Url Pool
** Url Collector
** Crawl Loop
** Privacy Context
** Proxy Management
** Web Drivers
** Web Driver Stealth
** Page Cache
** Backend Storage
* How to rotate my ip addresses?
* How to hide my bot from being detected?
* How & why to simulate human behaviors?
* How to render as many pages as possible on a single machine without be blocked?
