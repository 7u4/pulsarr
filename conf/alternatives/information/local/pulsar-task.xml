<?xml version="1.0" ?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl" ?>

<!-- Put site-specific property overrides in this file. -->

<configuration>

  <property>
    <name>pulsar.config.id</name>
    <value>local</value>
  </property>

  <!-- persist -->
  <property>
    <name>storage.crawl.id</name>
    <value>information_tmp_test_jobs</value>
    <description>Crawl pages for opinion mining</description>
  </property>

  <!-- net -->
  <property>
    <name>pulsar.master.host</name>
    <value>localhost</value>
  </property>

  <property>
    <name>pulsar.master.hostname</name>
    <value>localhost</value>
  </property>

  <!-- crawl -->
  <property>
    <name>crawl.max.distance</name>
    <value>2</value>
  </property>

  <!-- fetch -->
  <property>
    <name>fetch.threads.fetch</name>
    <value>5</value>
  </property>

  <property>
    <name>fetcher.threads.per.queue</name>
    <value>-1</value>
    <description>This number is the maximum number of threads that
      should be allowed to access a queue at one time. Setting it to
      a value > 1 will cause the Crawl-Delay value from robots.txt to
      be ignored and the value of fetch.queue.min.delay to be used
      as a delay between successive requests to the same server instead
      of fetch.queue.delay.
    </description>
  </property>

  <property>
    <name>recent.days.window</name>
    <value>3600</value>
  </property>

  <property>
    <name>parse.min.anchor.length</name>
    <!--<value>8</value>-->
    <value>2</value>
  </property>
  <property>
    <name>parse.max.anchor.length</name>
    <value>40</value>
  </property>

  <property>
    <name>pulsar.upstream.url</name>
    <value>http://master:8182/api</value>
  </property>

  <property>
    <name>indexer.url</name>
    <value>http://master:8983/information_tmp</value>
  </property>

  <property>
    <name>mapreduce.job.reduces</name>
    <value>1</value>
  </property>

</configuration>
